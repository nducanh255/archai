code:
  local_dir: ./
data:
  remote_dir: dataroot
  storage_id: us_east
description: Train Transformer XL
environment:
  image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel
  setup:
  - set -e -o xtrace
  - sudo apt-get -y install git
  - pip install --user tensorboard
search:
  job_template:
    command:
    - set -e -o xtrace
    - bash scripts/apex_install.sh
    - pip install --user -e .
    - python -m torch.distributed.launch --nproc_per_node="4" archai/nlp/nvidia_transformer_xl/train.py                                         --config
      {config} --config_file wt103_base.yaml                                         --n_layer
      {n_layer} --n_head {n_head} --d_model {d_model} --d_head {d_head}                                         --d_inner
      {d_inner} --d_embed {d_embed} --div_val {div_val}                                         --max_step
      500 --experiment_name config_86_500 --scheduler constant
    - python -m torch.distributed.launch --nproc_per_node="4" archai/nlp/nvidia_transformer_xl/train.py                                         --config
      {config} --config_file wt103_base.yaml                                         --n_layer
      {n_layer} --n_head {n_head} --d_model {d_model} --d_head {d_head}                                         --d_inner
      {d_inner} --d_embed {d_embed} --div_val {div_val}                                         --max_step
      1000 --experiment_name config_86_1000 --scheduler constant
    - python -m torch.distributed.launch --nproc_per_node="4" archai/nlp/nvidia_transformer_xl/train.py                                         --config
      {config} --config_file wt103_base.yaml                                         --n_layer
      {n_layer} --n_head {n_head} --d_model {d_model} --d_head {d_head}                                         --d_inner
      {d_inner} --d_embed {d_embed} --div_val {div_val}                                         --max_step
      1500 --experiment_name config_86_1500 --scheduler constant
    - python -m torch.distributed.launch --nproc_per_node="4" archai/nlp/nvidia_transformer_xl/train.py                                         --config
      {config} --config_file wt103_base.yaml                                         --n_layer
      {n_layer} --n_head {n_head} --d_model {d_model} --d_head {d_head}                                         --d_inner
      {d_inner} --d_embed {d_embed} --div_val {div_val}                                         --max_step
      2000 --experiment_name config_86_2000 --scheduler constant
    - python -m torch.distributed.launch --nproc_per_node="4" archai/nlp/nvidia_transformer_xl/train.py                                         --config
      {config} --config_file wt103_base.yaml                                         --n_layer
      {n_layer} --n_head {n_head} --d_model {d_model} --d_head {d_head}                                         --d_inner
      {d_inner} --d_embed {d_embed} --div_val {div_val}                                         --max_step
      2500 --experiment_name config_86_2500 --scheduler constant
    - python -m torch.distributed.launch --nproc_per_node="4" archai/nlp/nvidia_transformer_xl/train.py                                         --config
      {config} --config_file wt103_base.yaml                                         --n_layer
      {n_layer} --n_head {n_head} --d_model {d_model} --d_head {d_head}                                         --d_inner
      {d_inner} --d_embed {d_embed} --div_val {div_val}                                         --max_step
      3000 --experiment_name config_86_3000 --scheduler constant
    - python -m torch.distributed.launch --nproc_per_node="4" archai/nlp/nvidia_transformer_xl/train.py                                         --config
      {config} --config_file wt103_base.yaml                                         --n_layer
      {n_layer} --n_head {n_head} --d_model {d_model} --d_head {d_head}                                         --d_inner
      {d_inner} --d_embed {d_embed} --div_val {div_val}                                         --max_step
      3500 --experiment_name config_86_3500 --scheduler constant
    - python -m torch.distributed.launch --nproc_per_node="4" archai/nlp/nvidia_transformer_xl/train.py                                         --config
      {config} --config_file wt103_base.yaml                                         --n_layer
      {n_layer} --n_head {n_head} --d_model {d_model} --d_head {d_head}                                         --d_inner
      {d_inner} --d_embed {d_embed} --div_val {div_val}                                         --max_step
      4000 --experiment_name config_86_4000 --scheduler constant
    - python -m torch.distributed.launch --nproc_per_node="4" archai/nlp/nvidia_transformer_xl/train.py                                         --config
      {config} --config_file wt103_base.yaml                                         --n_layer
      {n_layer} --n_head {n_head} --d_model {d_model} --d_head {d_head}                                         --d_inner
      {d_inner} --d_embed {d_embed} --div_val {div_val}                                         --max_step
      4500 --experiment_name config_86_4500 --scheduler constant
    name: train_xl_config_86_500-5000
    sku: G4
  max_trials: 8
  params:
  - name: config
    spec: discrete
    values:
    - dgx1_4gpu_fp32
  - name: n_layer
    spec: discrete
    values:
    - 7
  - name: n_head
    spec: discrete
    values:
    - 2,2,8,8,2,4,2
  - name: d_model
    spec: discrete
    values:
    - 512
  - name: d_embed
    spec: discrete
    values:
    - 128
  - name: div_val
    spec: discrete
    values:
    - 1
  - name: d_head
    spec: discrete
    values:
    - 256,256,64,64,256,128,256
  - name: d_inner
    spec: discrete
    values:
    - 1034,1942,1141,1043,1923,1334,1596
  type: grid
storage:
  us_east:
    container_name: phillytools
    storage_account_name: archaieast
target:
  name: itplabrr1cl1
  service: amlk8s
  vc: resrchvc
