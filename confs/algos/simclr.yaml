# __include__: "../datasets/cifar10.yaml" # default dataset settings are for cifar

common:
  experiment_name: 'SimClr_Cifar10' # you should supply from command line
  experiment_desc: 'SimClr_Cifar10'
  # expdir: '~/logdir/models'
  logdir: '~/logdir/'
  log_prefix: 'log' # prefix for log files that will becreated (log.log and log.yaml), no log files if ''
  log_level: 20 # logging.INFO
  backup_existing_log_file: False # should we overwrite existing log file without making a copy?
  yaml_log: True # if True, structured logs as yaml are also generated
  seed: 2.0
  tb_enable: False # if True then TensorBoard logging is enabled (may impact perf)
  tb_dir: '$expdir/tb' # path where tensorboard logs would be stored
  save_delay: 30
  resume: False
  resumedir: ''
  save_intermediate: False
  intermediatedir: ''
  checkpoint:
    filename: '$expdir/checkpoint.pth'
    freq: 10
    experiment_name: '_copy: /common/experiment_name'
    resume: '_copy: /common/resume'
    resumedir: '_copy: /common/resumedir'
    save_intermediate: '_copy: /common/save_intermediate'
    intermediatedir: '_copy: /common/intermediatedir'

  # reddis address of Ray cluster. Use None for single node run
  # otherwise it should something like host:6379. Make sure to run on head node:
  # "ray start --head --redis-port=6379"
  redis: null
  apex: # this is overriden in search and eval individually
    enabled: False # global switch to disable everything apex
    distributed_enabled: True # enable/disable distributed mode
    mixed_prec_enabled: False # switch to disable amp mixed precision
    gpus: '' # use GPU IDs specified here (comma separated), if '' then use all GPUs
    opt_level: 'O2' # optimization level for mixed precision
    bn_fp32: False # keep BN in fp32
    loss_scale: "dynamic" # loss scaling mode for mixed prec, must be string reprenting floar ot "dynamic"
    sync_bn: False # should be replace BNs with sync BNs for distributed model
    scale_lr: False # enable/disable distributed mode
    min_world_size: 0 # allows to confirm we are indeed in distributed setting
    detect_anomaly: False # if True, PyTorch code will run 6X slower
    seed: '_copy: /common/seed'
    ray:
      enabled: False # initialize ray. Note: ray cannot be used if apex distributed is enabled
      local_mode: False # if True then ray runs in serial mode
    resume: '_copy: /common/checkpoint/resume'

  smoke_test: False
  only_eval: False


loader:
  apex:
    _copy: '../../trainer/apex'
  aug: '' # additional augmentations to use, for ex, fa_reduced_cifar10, arsaug, autoaug_cifar10, autoaug_extend
  cutout: 0 # cutout length, use cutout augmentation when > 0
  load_train: True # load train split of dataset
  train_batch: 512 
  train_workers: 4
  test_workers: '_copy: ../train_workers' # if null then 4
  load_test: True # load test split of dataset
  test_batch: 1024
  val_ratio: 0.0 #split portion for test set, 0 to 1
  val_fold: 0 #Fold number to use (0 to 4)
  cv_num: 5 # total number of folds available
  dataset:
    _copy: '/dataset'
    jitter_strength: 0.5
    input_height: 32
    gaussian_blur: False
    normalize: True

trainer:
  title: 'eval_train'
  resume: '_copy: /common/checkpoint/resume'
  apex:
    _copy: '/common/apex'
  epochs: 300
  logger_freq: 10
  drop_path_prob: 0.0
  grad_clip: 0.0
  aux_weight: 0.0
  batch_chunks: 1 # split batch into these many chunks and accumulate gradients so we can support GPUs with lower RAM
  model: "resnet18"
  lossfn:
    type: 'NTXentLoss'
    temperature: 0.5
    eps: 1.0e-6
  optimizer:
    # type: 'adam'
    # lr: 1.0 # init learning rate
    # decay: 1.0e-4 #s pytorch default is 0.0
    # betas: [0.9,0.999] # pytorch default is 0.0
    # decay_bn: .NaN # if NaN then same as decay otherwise apply different decay to BN layers
    type: 'lars'
    lr: 5.0 # init learning rate
    decay: 1.0e-6 # pytorch default is 0.0
    momentum: 0.9 # pytorch default is 0.0
    trust_coeff: 0.001
    decay_bn: .NaN # if NaN then same as decay otherwise apply different decay to BN layers
  lr_schedule:
    type: 'cosine'
    min_lr: 1.0e-4 # min learning rate to se bet in eta_min param of scheduler
    warmup:  # increases LR for 0 to current in specified epochs and then hands over to main scheduler
      multiplier: 1
      epochs: 10 # 0 disables warmup
  validation:
    title: 'eval_test'
    batch_chunks: '_copy: ../../batch_chunks' # split batch into these many chunks and accumulate gradients so we can support GPUs with lower RAM
    logger_freq: 0
    freq: 10 # perform validation only every N epochs
    lossfn:
      type: 'NTXentLoss'
      temperature: 0.5
      eps: 1.0e-6

